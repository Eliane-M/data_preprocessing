{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment covers data preprocessing with datasets contaning overlapping but different features. The goal is to augment, merge, and enhance the data while ensuring consistency in a machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 1: Data Augmentation on CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "import pandas as pd\n",
    "\n",
    "file_path = '/initial_dataset/customer_transactions.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id_legacy</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>purchase_amount</th>\n",
       "      <th>purchase_date</th>\n",
       "      <th>product_category</th>\n",
       "      <th>customer_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>151</td>\n",
       "      <td>1001</td>\n",
       "      <td>408</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>Sports</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192</td>\n",
       "      <td>1002</td>\n",
       "      <td>332</td>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114</td>\n",
       "      <td>1003</td>\n",
       "      <td>442</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>171</td>\n",
       "      <td>1004</td>\n",
       "      <td>256</td>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160</td>\n",
       "      <td>1005</td>\n",
       "      <td>64</td>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>102</td>\n",
       "      <td>1146</td>\n",
       "      <td>88</td>\n",
       "      <td>2024-05-25</td>\n",
       "      <td>Sports</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>100</td>\n",
       "      <td>1147</td>\n",
       "      <td>387</td>\n",
       "      <td>2024-05-26</td>\n",
       "      <td>Books</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>104</td>\n",
       "      <td>1148</td>\n",
       "      <td>409</td>\n",
       "      <td>2024-05-27</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>189</td>\n",
       "      <td>1149</td>\n",
       "      <td>178</td>\n",
       "      <td>2024-05-28</td>\n",
       "      <td>Sports</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>113</td>\n",
       "      <td>1150</td>\n",
       "      <td>316</td>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     customer_id_legacy  transaction_id  purchase_amount purchase_date  \\\n",
       "0                   151            1001              408    2024-01-01   \n",
       "1                   192            1002              332    2024-01-02   \n",
       "2                   114            1003              442    2024-01-03   \n",
       "3                   171            1004              256    2024-01-04   \n",
       "4                   160            1005               64    2024-01-05   \n",
       "..                  ...             ...              ...           ...   \n",
       "145                 102            1146               88    2024-05-25   \n",
       "146                 100            1147              387    2024-05-26   \n",
       "147                 104            1148              409    2024-05-27   \n",
       "148                 189            1149              178    2024-05-28   \n",
       "149                 113            1150              316    2024-05-29   \n",
       "\n",
       "    product_category  customer_rating  \n",
       "0             Sports              2.3  \n",
       "1        Electronics              4.2  \n",
       "2        Electronics              2.1  \n",
       "3           Clothing              2.8  \n",
       "4           Clothing              1.3  \n",
       "..               ...              ...  \n",
       "145           Sports              2.7  \n",
       "146            Books              4.6  \n",
       "147         Clothing              1.4  \n",
       "148           Sports              3.0  \n",
       "149         Clothing              1.0  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_id_legacy     0\n",
      "transaction_id         0\n",
      "purchase_amount        0\n",
      "purchase_date          0\n",
      "product_category       0\n",
      "customer_rating       10\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18044\\1319517814.py:14: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  data['purchase_date'] = pd.to_datetime(data['purchase_date']).view('int64') / 10**9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating mean imputation...\n",
      "Mean MSE: 1.4167\n",
      "Mean R²: -0.1920\n",
      "\n",
      "Evaluating median imputation...\n",
      "Median MSE: 1.4067\n",
      "Median R²: -0.1800\n",
      "\n",
      "Evaluating most_frequent imputation...\n",
      "Most_frequent MSE: 4.8067\n",
      "Most_frequent R²: -0.1233\n",
      "\n",
      "Evaluating random_forest imputation...\n",
      "Random_forest MSE: 1.4318\n",
      "Random_forest R²: -0.1007\n",
      "\n",
      "Best Imputation Method: median with MSE: 1.4067\n",
      "Best imputer model saved as 'best_imputer.pkl'!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import joblib\n",
    "\n",
    "# Assuming 'data' is already loaded from 'customer_transactions.csv'\n",
    "# Select numeric columns for imputation (modify as needed)\n",
    "target_column = \"customer_rating\"\n",
    "features = data.drop(columns=[target_column])  # Excluding target\n",
    "\n",
    "\n",
    "data['purchase_date'] = pd.to_datetime(data['purchase_date']).view('int64') / 10**9\n",
    "# one hot encoding for product category\n",
    "data = pd.get_dummies(data, columns=['product_category'])\n",
    "\n",
    "# Create a dataset with missing values artificially\n",
    "data_missing = data.copy()\n",
    "mask = np.random.rand(len(data_missing)) < 0.1  # 10% missing values\n",
    "data_missing[target_column] = data_missing[target_column].mask(mask)\n",
    "\n",
    "# Ground truth for evaluation (original values before masking)\n",
    "y_true = data[target_column][mask]\n",
    "\n",
    "# Different imputation strategies\n",
    "strategies = ['mean', 'median', 'most_frequent', 'random_forest']\n",
    "results = {}\n",
    "\n",
    "# Impute and evaluate each strategy\n",
    "for strategy in strategies:\n",
    "    print(f\"\\nEvaluating {strategy} imputation...\")\n",
    "    data_imputed = data_missing.copy()\n",
    "\n",
    "    if strategy == 'random_forest':\n",
    "        # Predictive modeling with Random Forest\n",
    "        train_data = data_imputed[data_imputed[target_column].notnull()]\n",
    "        test_data = data_imputed[data_imputed[target_column].isnull()]\n",
    "        \n",
    "        X_train = train_data.drop(columns=[target_column])\n",
    "        y_train = train_data[target_column]\n",
    "        X_test = test_data.drop(columns=[target_column])\n",
    "\n",
    "        rf_imputer = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf_imputer.fit(X_train, y_train)\n",
    "        imputed_values = rf_imputer.predict(X_test)\n",
    "        \n",
    "        data_imputed.loc[data_imputed[target_column].isnull(), target_column] = imputed_values\n",
    "    else:\n",
    "        # Simple imputation (mean, median, mode)\n",
    "        imputer = SimpleImputer(strategy=strategy)\n",
    "        data_imputed[[target_column]] = imputer.fit_transform(data_imputed[[target_column]])\n",
    "\n",
    "    # Evaluate imputation performance using MSE against ground truth\n",
    "    y_pred = data_imputed[target_column][mask]\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    results[strategy] = mse\n",
    "    print(f\"{strategy.capitalize()} MSE: {mse:.4f}\")\n",
    "\n",
    "    # Optional: Train a model to check predictive power (R²)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data_imputed.drop(columns=[target_column]), data_imputed[target_column], \n",
    "        test_size=0.2, random_state=42\n",
    "    )\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    print(f\"{strategy.capitalize()} R²: {scores.mean():.4f}\")\n",
    "\n",
    "# Find the best imputation method\n",
    "best_strategy = min(results, key=results.get)  # Lower MSE is better\n",
    "print(f\"\\nBest Imputation Method: {best_strategy} with MSE: {results[best_strategy]:.4f}\")\n",
    "\n",
    "# Save the best imputer model\n",
    "if best_strategy == 'random_forest':\n",
    "    joblib.dump(rf_imputer, \"best_rf_imputer.pkl\")\n",
    "    print(\"Best Random Forest imputer saved as 'best_rf_imputer.pkl'!\")\n",
    "else:\n",
    "    best_imputer = SimpleImputer(strategy=best_strategy)\n",
    "    best_imputer.fit(data[[target_column]])  # Train on original data\n",
    "    joblib.dump(best_imputer, \"/models/best_imputer.pkl\")\n",
    "    print(\"Best imputer model saved as 'best_imputer.pkl'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_id_legacy              0\n",
      "transaction_id                  0\n",
      "purchase_amount                 0\n",
      "purchase_date                   0\n",
      "customer_rating                 0\n",
      "product_category_Books          0\n",
      "product_category_Clothing       0\n",
      "product_category_Electronics    0\n",
      "product_category_Groceries      0\n",
      "product_category_Sports         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the saved imputer model\n",
    "best_imputer = joblib.load(\"/models/best_imputer.pkl\")\n",
    "\n",
    "# Apply the imputer to fill missing values\n",
    "data = data.copy()  # Make a copy of the original data\n",
    "data[['customer_rating']] = best_imputer.transform(data[['customer_rating']])\n",
    "\n",
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18044\\1243049603.py:7: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  data['purchase_date'] = pd.to_datetime(data['purchase_date']).view('int64') / 10**9\n",
      "c:\\Users\\user\\data_preprocessing\\venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying random noise to numerical transaction values...\n",
      "Random noise applied.\n",
      "Created synthetic target 'high_value_customer' based on customer_rating > 2.5.\n",
      "Applying SMOTE to balance the dataset...\n",
      "SMOTE applied. New shape: (196, 7)\n",
      "\n",
      "Applying log transformation to skewed numerical data...\n",
      "No transformation needed for purchase_amount (skewness < 1)\n",
      "No transformation needed for purchase_date (skewness < 1)\n",
      "No transformation needed for customer_rating (skewness < 1)\n",
      "No transformation needed for high_value_customer (skewness < 1)\n",
      "\n",
      "Generating synthetic transactions...\n",
      "Synthetic transactions added. New shape: (235, 7)\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation Strategies\n",
    "\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.stats import skew\n",
    "\n",
    "data['purchase_date'] = pd.to_datetime(data['purchase_date']).view('int64') / 10**9\n",
    "# numerical columns\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols = [col for col in numeric_cols if col not in ['customer_id_legacy', 'transaction_id']]\n",
    "\n",
    "# Synthetic data generation: using random noise and SMOTE\n",
    "# Random noise\n",
    "print(\"Applying random noise to numerical transaction values...\")\n",
    "for col in numeric_cols:\n",
    "    noise = np.random.normal(0, 0.05 * data[col].std(), size=data[col].shape)  # 5% of std as noise\n",
    "    data[col] = data[col] + noise\n",
    "print(\"Random noise applied.\")\n",
    "\n",
    "data['high_value_customer'] = (data['customer_rating'] > 2.5).astype(int)\n",
    "print(\"Created synthetic target 'high_value_customer' based on customer_rating > 2.5.\")\n",
    "\n",
    "# Use SMOTE to balance the dataset\n",
    "print(\"Applying SMOTE to balance the dataset...\")\n",
    "X = data.drop(columns=['high_value_customer', 'customer_id_legacy', 'transaction_id'])\n",
    "y = data['high_value_customer']\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Reconstruct the dataset after SMOTE\n",
    "data_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "data_resampled['high_value_customer'] = y_resampled\n",
    "\n",
    "# Decode 'product_category' back to its original form\n",
    "category_cols = [col for col in data_resampled.columns if col.startswith('product_')]\n",
    "if not category_cols:\n",
    "    raise ValueError(\"No product_ columns found in data_resampled.\")\n",
    "\n",
    "# Find category with highest value (one-hot encoded column)\n",
    "category_labels = data_resampled[category_cols].idxmax(axis=1)\n",
    "category_labels = category_labels.str.replace('product_', '', regex=True)\n",
    "\n",
    "# Assign back to dataset\n",
    "data_resampled['product_category'] = category_labels\n",
    "data_resampled = data_resampled.drop(columns=category_cols)\n",
    "\n",
    "# Add back 'customer_id_legacy' and 'transaction_id' (assign placeholders for new rows)\n",
    "data_resampled['customer_id_legacy'] = -1  # Placeholder for new rows\n",
    "data_resampled['transaction_id'] = range(len(data_resampled))  # Sequential IDs\n",
    "data = data_resampled[['customer_id_legacy', 'transaction_id', 'purchase_amount', \n",
    "                       'purchase_date', 'product_category', 'customer_rating', 'high_value_customer']]\n",
    "print(\"SMOTE applied. New shape:\", data.shape)\n",
    "\n",
    "# Feature value transformation\n",
    "print(\"\\nApplying log transformation to skewed numerical data...\")\n",
    "for col in numeric_cols:\n",
    "    if skew(data[col].dropna()) > 1:  # Check skewness\n",
    "        data[col] = np.log1p(data[col].clip(lower=0))  # Clip to avoid negatives, then log\n",
    "        print(f\"Log transformation applied to {col}\")\n",
    "    else:\n",
    "        print(f\"No transformation needed for {col} (skewness < 1)\")\n",
    "\n",
    "\n",
    "# Data expansion\n",
    "print(\"\\nGenerating synthetic transactions...\")\n",
    "new_transactions = data.sample(frac=0.2, random_state=42)  # 20% of original data as synthetic\n",
    "for col in numeric_cols:\n",
    "    new_transactions[col] = new_transactions[col] * (1 + np.random.uniform(-0.1, 0.1))  # ±10% variation\n",
    "data_augmented = pd.concat([data, new_transactions], ignore_index=True)\n",
    "print(\"Synthetic transactions added. New shape:\", data_augmented.shape)\n",
    "\n",
    "\n",
    "data_augmented = data_augmented.drop(columns=['high_value_customer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exporting augmented dataset...\n",
      "Augmented dataset saved as 'customer_transactions_augmented.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Save the augmented dataset\n",
    "\n",
    "print(\"\\nExporting augmented dataset...\")\n",
    "data_augmented.to_csv('/augmented_dataset/customer_transactions_augmented.csv', index=False)\n",
    "print(\"Augmented dataset saved as 'customer_transactions_augmented.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
