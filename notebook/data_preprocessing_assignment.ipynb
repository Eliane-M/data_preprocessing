{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment covers data preprocessing with datasets contaning overlapping but different features. The goal is to augment, merge, and enhance the data while ensuring consistency in a machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 1: Data Augmentation on CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "import pandas as pd\n",
    "\n",
    "file_path = '../initial_dataset/customer_transactions.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id_legacy</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>purchase_amount</th>\n",
       "      <th>purchase_date</th>\n",
       "      <th>product_category</th>\n",
       "      <th>customer_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>151</td>\n",
       "      <td>1001</td>\n",
       "      <td>408</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>Sports</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192</td>\n",
       "      <td>1002</td>\n",
       "      <td>332</td>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114</td>\n",
       "      <td>1003</td>\n",
       "      <td>442</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>171</td>\n",
       "      <td>1004</td>\n",
       "      <td>256</td>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160</td>\n",
       "      <td>1005</td>\n",
       "      <td>64</td>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>102</td>\n",
       "      <td>1146</td>\n",
       "      <td>88</td>\n",
       "      <td>2024-05-25</td>\n",
       "      <td>Sports</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>100</td>\n",
       "      <td>1147</td>\n",
       "      <td>387</td>\n",
       "      <td>2024-05-26</td>\n",
       "      <td>Books</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>104</td>\n",
       "      <td>1148</td>\n",
       "      <td>409</td>\n",
       "      <td>2024-05-27</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>189</td>\n",
       "      <td>1149</td>\n",
       "      <td>178</td>\n",
       "      <td>2024-05-28</td>\n",
       "      <td>Sports</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>113</td>\n",
       "      <td>1150</td>\n",
       "      <td>316</td>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     customer_id_legacy  transaction_id  ...  product_category customer_rating\n",
       "0                   151            1001  ...            Sports             2.3\n",
       "1                   192            1002  ...       Electronics             4.2\n",
       "2                   114            1003  ...       Electronics             2.1\n",
       "3                   171            1004  ...          Clothing             2.8\n",
       "4                   160            1005  ...          Clothing             1.3\n",
       "..                  ...             ...  ...               ...             ...\n",
       "145                 102            1146  ...            Sports             2.7\n",
       "146                 100            1147  ...             Books             4.6\n",
       "147                 104            1148  ...          Clothing             1.4\n",
       "148                 189            1149  ...            Sports             3.0\n",
       "149                 113            1150  ...          Clothing             1.0\n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_id_legacy     0\n",
      "transaction_id         0\n",
      "purchase_amount        0\n",
      "purchase_date          0\n",
      "product_category       0\n",
      "customer_rating       10\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# Handle missing values\n",
    "# Numerical columns: Impute with mean (purchase_amount, customer_rating)\n",
    "num_cols = ['purchase_amount', 'customer_rating']\n",
    "\n",
    "# Impute numerical columns with mean\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "data[num_cols] = num_imputer.fit_transform(data[num_cols])\n",
    "\n",
    "# Categorical columns: Impute with mode (product_category)\n",
    "cat_cols = ['product_category']\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "data[cat_cols] = cat_imputer.fit_transform(data[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation Strategies\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Apply random noise to purchase_amount (±5%)\n",
    "data['purchase_amount'] = data['purchase_amount'] * (1 + np.random.normal(0, 0.05, data.shape[0]))  # ±5% noise\n",
    "\n",
    "# Apply log transformation to purchase_amount (log(1 + x))\n",
    "data['purchase_amount'] = np.log1p(data['purchase_amount'])\n",
    "\n",
    "# Expand data by generating synthetic transactions (slightly modify the existing ones)\n",
    "synthetic_data = data.copy()\n",
    "\n",
    "# Add slight variations to the purchase_amount and customer_rating for synthetic data\n",
    "synthetic_data['purchase_amount'] *= np.random.uniform(0.9, 1.1, synthetic_data.shape[0])  # ±10% variation\n",
    "synthetic_data['transaction_id'] = synthetic_data['transaction_id'].astype(str) + \"_synth\"  # Mark synthetic data\n",
    "\n",
    "# Append the synthetic data to the original dataset\n",
    "data = pd.concat([data, synthetic_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exporting augmented dataset...\n",
      "Augmented dataset saved as 'customer_transactions_augmented.csv'.\n",
      "     customer_id_legacy transaction_id  purchase_amount purchase_date  \\\n",
      "0                   151           1001         6.005589    2024-01-01   \n",
      "1                   192           1002         5.767591    2024-01-02   \n",
      "2                   114           1003         5.985804    2024-01-03   \n",
      "3                   171           1004         5.578930    2024-01-04   \n",
      "4                   160           1005         4.177239    2024-01-05   \n",
      "..                  ...            ...              ...           ...   \n",
      "295                 102     1146_synth         4.489214    2024-05-25   \n",
      "296                 100     1147_synth         6.533912    2024-05-26   \n",
      "297                 104     1148_synth         6.185791    2024-05-27   \n",
      "298                 189     1149_synth         5.488780    2024-05-28   \n",
      "299                 113     1150_synth         5.125929    2024-05-29   \n",
      "\n",
      "    product_category  customer_rating  \n",
      "0             Sports              2.3  \n",
      "1        Electronics              4.2  \n",
      "2        Electronics              2.1  \n",
      "3           Clothing              2.8  \n",
      "4           Clothing              1.3  \n",
      "..               ...              ...  \n",
      "295           Sports              2.7  \n",
      "296            Books              4.6  \n",
      "297         Clothing              1.4  \n",
      "298           Sports              3.0  \n",
      "299         Clothing              1.0  \n",
      "\n",
      "[300 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Save the augmented dataset\n",
    "\n",
    "print(\"\\nExporting augmented dataset...\")\n",
    "data.to_csv('../augmented_dataset/customer_transactions_augmented.csv', index=False)\n",
    "print(\"Augmented dataset saved as 'customer_transactions_augmented.csv'.\")\n",
    "\n",
    "augmented_data = pd.read_csv('../augmented_dataset/customer_transactions_augmented.csv')\n",
    "print(augmented_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
