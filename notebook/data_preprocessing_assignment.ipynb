{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment covers data preprocessing with datasets contaning overlapping but different features. The goal is to augment, merge, and enhance the data while ensuring consistency in a machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 1: Data Augmentation on CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Loading the dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../initial_dataset/customer_transactions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "import pandas as pd\n",
    "\n",
    "file_path = '../initial_dataset/customer_transactions.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id_legacy</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>purchase_amount</th>\n",
       "      <th>purchase_date</th>\n",
       "      <th>product_category</th>\n",
       "      <th>customer_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>151</td>\n",
       "      <td>1001</td>\n",
       "      <td>408</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>Sports</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192</td>\n",
       "      <td>1002</td>\n",
       "      <td>332</td>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114</td>\n",
       "      <td>1003</td>\n",
       "      <td>442</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>171</td>\n",
       "      <td>1004</td>\n",
       "      <td>256</td>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160</td>\n",
       "      <td>1005</td>\n",
       "      <td>64</td>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>102</td>\n",
       "      <td>1146</td>\n",
       "      <td>88</td>\n",
       "      <td>2024-05-25</td>\n",
       "      <td>Sports</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>100</td>\n",
       "      <td>1147</td>\n",
       "      <td>387</td>\n",
       "      <td>2024-05-26</td>\n",
       "      <td>Books</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>104</td>\n",
       "      <td>1148</td>\n",
       "      <td>409</td>\n",
       "      <td>2024-05-27</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>189</td>\n",
       "      <td>1149</td>\n",
       "      <td>178</td>\n",
       "      <td>2024-05-28</td>\n",
       "      <td>Sports</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>113</td>\n",
       "      <td>1150</td>\n",
       "      <td>316</td>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     customer_id_legacy  transaction_id  purchase_amount purchase_date  \\\n",
       "0                   151            1001              408    2024-01-01   \n",
       "1                   192            1002              332    2024-01-02   \n",
       "2                   114            1003              442    2024-01-03   \n",
       "3                   171            1004              256    2024-01-04   \n",
       "4                   160            1005               64    2024-01-05   \n",
       "..                  ...             ...              ...           ...   \n",
       "145                 102            1146               88    2024-05-25   \n",
       "146                 100            1147              387    2024-05-26   \n",
       "147                 104            1148              409    2024-05-27   \n",
       "148                 189            1149              178    2024-05-28   \n",
       "149                 113            1150              316    2024-05-29   \n",
       "\n",
       "    product_category  customer_rating  \n",
       "0             Sports              2.3  \n",
       "1        Electronics              4.2  \n",
       "2        Electronics              2.1  \n",
       "3           Clothing              2.8  \n",
       "4           Clothing              1.3  \n",
       "..               ...              ...  \n",
       "145           Sports              2.7  \n",
       "146            Books              4.6  \n",
       "147         Clothing              1.4  \n",
       "148           Sports              3.0  \n",
       "149         Clothing              1.0  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_id_legacy     0\n",
      "transaction_id         0\n",
      "purchase_amount        0\n",
      "purchase_date          0\n",
      "product_category       0\n",
      "customer_rating       10\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# Handle missing values\n",
    "# Numerical columns: Impute with mean (purchase_amount, customer_rating)\n",
    "num_cols = ['purchase_amount', 'customer_rating']\n",
    "\n",
    "# Impute numerical columns with mean\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "data[num_cols] = num_imputer.fit_transform(data[num_cols])\n",
    "\n",
    "# Categorical columns: Impute with mode (product_category)\n",
    "cat_cols = ['product_category']\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "data[cat_cols] = cat_imputer.fit_transform(data[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation Strategies\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Apply random noise to purchase_amount (±5%)\n",
    "data['purchase_amount'] = data['purchase_amount'] * (1 + np.random.normal(0, 0.05, data.shape[0]))  # ±5% noise\n",
    "\n",
    "# Apply log transformation to purchase_amount (log(1 + x))\n",
    "data['purchase_amount'] = np.log1p(data['purchase_amount'])\n",
    "\n",
    "# Expand data by generating synthetic transactions (slightly modify the existing ones)\n",
    "synthetic_data = data.copy()\n",
    "\n",
    "# Add slight variations to the purchase_amount and customer_rating for synthetic data\n",
    "synthetic_data['purchase_amount'] *= np.random.uniform(0.9, 1.1, synthetic_data.shape[0])  # ±10% variation\n",
    "synthetic_data['transaction_id'] = synthetic_data['transaction_id'].astype(str) + \"_synth\"  # Mark synthetic data\n",
    "\n",
    "# Append the synthetic data to the original dataset\n",
    "data = pd.concat([data, synthetic_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exporting augmented dataset...\n",
      "Augmented dataset saved as 'customer_transactions_augmented.csv'.\n",
      "     customer_id_legacy transaction_id  purchase_amount purchase_date  \\\n",
      "0                   151           1001         6.005589    2024-01-01   \n",
      "1                   192           1002         5.767591    2024-01-02   \n",
      "2                   114           1003         5.985804    2024-01-03   \n",
      "3                   171           1004         5.578930    2024-01-04   \n",
      "4                   160           1005         4.177239    2024-01-05   \n",
      "..                  ...            ...              ...           ...   \n",
      "295                 102     1146_synth         4.489214    2024-05-25   \n",
      "296                 100     1147_synth         6.533912    2024-05-26   \n",
      "297                 104     1148_synth         6.185791    2024-05-27   \n",
      "298                 189     1149_synth         5.488780    2024-05-28   \n",
      "299                 113     1150_synth         5.125929    2024-05-29   \n",
      "\n",
      "    product_category  customer_rating  \n",
      "0             Sports              2.3  \n",
      "1        Electronics              4.2  \n",
      "2        Electronics              2.1  \n",
      "3           Clothing              2.8  \n",
      "4           Clothing              1.3  \n",
      "..               ...              ...  \n",
      "295           Sports              2.7  \n",
      "296            Books              4.6  \n",
      "297         Clothing              1.4  \n",
      "298           Sports              3.0  \n",
      "299         Clothing              1.0  \n",
      "\n",
      "[300 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Save the augmented dataset\n",
    "\n",
    "print(\"\\nExporting augmented dataset...\")\n",
    "data.to_csv('../augmented_dataset/customer_transactions_augmented.csv', index=False)\n",
    "print(\"Augmented dataset saved as 'customer_transactions_augmented.csv'.\")\n",
    "\n",
    "augmented_data = pd.read_csv('../augmented_dataset/customer_transactions_augmented.csv')\n",
    "print(augmented_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv(\"../augmented_dataset/customer_transactions_augmented.csv\")\n",
    "social_profiles_df = pd.read_csv(\"../initial_dataset/customer_social_profiles.csv\")\n",
    "id_mapping_df = pd.read_csv(\"../initial_dataset/id_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.columns\n",
    "social_profiles_df.columns\n",
    "id_mapping_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a complex merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Clean ID Mapping\n",
    "id_mapping_df = id_mapping_df.drop_duplicates(subset=['customer_id_legacy'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Merge transactions with ID mapping\n",
    "merged_df = transactions_df.merge(id_mapping_df, on=\"customer_id_legacy\", how=\"left\")\n",
    "\n",
    "# 2: Merge with social media profiles\n",
    "final_df = merged_df.merge(social_profiles_df, on=\"customer_id_new\", how=\"left\")\n",
    "\n",
    "# Handle missing social profiles (customers without social media data)\n",
    "final_df.fillna({\n",
    "    \"social_media_platform\": \"Unknown\",\n",
    "    \"engagement_score\": final_df[\"engagement_score\"].median(),\n",
    "    \"purchase_interest_score\": final_df[\"purchase_interest_score\"].median(),\n",
    "    \"review_sentiment\": \"Neutral\"\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(final_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering & Tansformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Customer Engagement Score\n",
    "final_df[\"customer_engagement_score\"] = (\n",
    "    0.6 * final_df[\"engagement_score\"] + 0.4 * final_df[\"purchase_interest_score\"])\n",
    "\n",
    "final_df[\"purchase_date\"] = pd.to_datetime(final_df[\"purchase_date\"])\n",
    "# Compute Moving Average of Last 3 Transactions\n",
    "final_df[\"moving_avg_purchase\"] = final_df.groupby(\"customer_id_legacy\")[\"purchase_amount\"].transform(lambda x: x.rolling(3, min_periods=1).mean())\n",
    "\n",
    "# Aggregate Monthly Spending\n",
    "final_df[\"purchase_month\"] = final_df[\"purchase_date\"].dt.to_period(\"M\")\n",
    "monthly_spending = final_df.groupby([\"customer_id_legacy\", \"purchase_month\"])[\"purchase_amount\"].agg([\"sum\", \"mean\"]).reset_index()\n",
    "monthly_spending.rename(columns={\"sum\": \"monthly_total_spend\", \"mean\": \"monthly_avg_spend\"}, inplace=True)\n",
    "\n",
    "# Merge Monthly Spending Data\n",
    "final_df = final_df.merge(monthly_spending, on=[\"customer_id_legacy\", \"purchase_month\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.get_dummies(final_df, columns=['product_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10)\n",
    "tfidf_matrix = vectorizer.fit_transform(final_df['review_sentiment'].fillna(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(final_df.duplicated().sum())  # Count duplicate rows\n",
    "print(final_df.isnull().sum())\n",
    "final_df.info()  # Count missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'moving_avg_purchase' with the actual column if different\n",
    "final_df = final_df.drop_duplicates()\n",
    "final_df['avg_spent_last_3'] = final_df['moving_avg_purchase']  # Create the column\n",
    "final_df['avg_spent_last_3'] = final_df['avg_spent_last_3'].fillna(final_df['avg_spent_last_3'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.histplot(final_df['purchase_amount'], bins=50, kde=True)\n",
    "plt.title(\"Distribution of Purchase Amounts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns for correlation\n",
    "df_numeric = final_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df_numeric.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_df.to_csv(\"final_customer_data_10.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
